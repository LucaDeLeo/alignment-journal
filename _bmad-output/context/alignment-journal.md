---
created: 2026-02-06T16:06:29 (UTC -03:00)
tags: []
source: https://woozy-page-39c.notion.site/Alignment-Journal-Description-Jan-2026-2e9398af2ab0816ea6cdf847f6447b46
author: 
---

# Alignment Journal Description (Jan 2026)

> ## Excerpt
> Executive Summary

---
This is the description of the main features of and motivation for the Alignment journal. Frozen snapshot January 2026. You can share this privately but please don’t post on social media.

The journal Alignment will be a fast and rigorous venue for theoretical AI alignment—research on agency, understanding, and asymptotic behavior of advanced and potentially self-modifying synthetic agents. It targets a gap left by machine learning conferences, and will avoid deficiencies of journals, conferences, and online forums: existing academic venues are slow, opaque, and waste expert effort, while forums lack sufficient depth, filtering, and legible certification. Our goals are to accelerate progress; integrate academia, labs, and independents; help funders and readers distinguish genuine alignment research; and raise paper quality and readability, especially for independent researchers.

These key features will combine the strengths of journals, conferences, workshops, and internet forums while avoiding their pathologies:

reviewers drawn from the entire research community, rather than a limited and contingent conference pool, and matched carefully for expertise;

a “[reviewer abstract](https://woozy-page-39c.notion.site/2e9398af2ab0816ea6cdf847f6447b46?pvs=25#2e9398af2ab08184aaa1e2f9d411e99f)” published alongside accepted papers to convey value, caveats, target audience, and improve cross-disciplinary dissemination of important results;

[paid peer review](https://woozy-page-39c.notion.site/2e9398af2ab0816ea6cdf847f6447b46?pvs=25#2e9398af2ab081b28b3ef31f11296bbb) with incentives for quality, speed, and writing the reviewer abstract;

rolling submissions with periodic Editor’s Selections to highlight the best work and create useful deadlines.

The publication pipeline for academics and for AI safety researchers are ripe for disruption; there are many roadblocks to effective research that can be addressed with a well-designed intervention. Currently, alignment research is scattered across multiple places depending upon emphasis, each of which has different shortcomings, and none of which can make a strong claim to represent a canonical destination of alignment research.

Firstly, consider classic journals. Although these still publish alignment-relevant research across diverse domains, such as mathematics, politics, philosophy, computer science, neuroscience, and engineering, they suffer from the classic problems of journals in general, to wit slowness, expense, and conservatism.

Secondly, consider machine learning conferences. These have some AI safety and alignment content, but they are dominated by the needs and interests of large tech companies, and are less likely to publish research on topics far from the AI acceleration profit centers. Although faster and more experimental than legacy journals, their publication cycle is still relatively slow, and they do not include research except from relatively narrowly defined areas of computer science.

Thirdly, we have workshops and symposia (conference-affiliated or independent), which publish high-quality alignment research, but have poor quality filters and are expensive to attend, and which do not themselves count towards career progression since they are not publications per se. Nor is a workshop paper typically useful in public science communication of AI risks, which prefers peer-reviewed research.

Finally, consider the world of AI safety research online, such as the Alignment Forum and the many preprints and tech reports available from the open-source, philanthropic research institute-backed NGOs. These are extremely rapid. However their feedback mechanisms are weak and illegible, in particular often lacking deep expert review. Since they are regarded as informal by institutional academia, time spent on such outputs is dead time, from the perspective of institutional research performances indicators and career progression.

Our proposal aims to simultaneously remedy the problems with all these modes of publishing AI safety and alignment research. Our bet is that we can could unify and expand the field of alignment by establishing a legitimate academic journal with an unorthodox review pipeline. By experimenting simultaneously with the peer-review process and with academic incentives, we seek the best features existing avenues of alignment research and minimize exposure to the legacy costs, thereby accelerating research in the field.

The result would in the best case attain the speed and high-quality feedback of a machine-learning conference, the speed of online report publications, and the institutional legitimacy of legacy journals. This ideally leads to better mixing of communities of research, faster turnaround and a greater legitimacy for all participants, thus incentivising an expansion and acceleration of the field of AI Safety research.

Build a venue for improving and disseminating research on AI alignment that emphasizes conceptual understanding and de-emphasizes capabilities obtained through empirical hill climbing

Accelerate alignment research by combining the best features from traditional journals, conference proceedings, internet forums, and social media

More accurately match papers to the best reviewers based on expertise and interest, making review more rigorous and carefully deploying scarce expert attention

Bring researchers from traditional academia, frontier labs, and independent organizations into communion

Help alignment researchers distinguish themselves from capabilities researchers, and assist research funders in identifying such researchers

Filter and make legible the top alignment research results to outsiders

Provide a rigorously vetted, high-standards venue incentivizing high-quality contributions from researchers outside the academy (including commercial labs and independent researchers)

The Alignment journal publishes theoretical research on the agency, understanding, and asymptotic behavior of artificial intelligent systems in unconstrained environments, especially conceptual and mathematically abstract ideas that transcend contemporary architectures and potentially apply to horizon technologies such as powerful autonomous and self-modifying agents. The primary acceptance criteria are technical correctness and interest to the technical AI alignment research community, so we will more broadly consider work — such as numerical, translational, and interdisciplinary research — that informs the theoretical topics above.

Although we welcome work that can demonstrate its results empirically, we differ from many mainstream machine learning conferences in placing less emphasis on state-of-the-art empirics on standard benchmarks. For the time being, most research on AI governance, deployment, applied mechanistic interpretability, evaluations, and societal impact is out of scope, although this is subject to revision.

If you would like to know if your work is in scope, email a manuscript PDF to the editors and we will try give you feedback within 48 hours.

The rest of this document describes the proposed Alignment journal in more detail. Here we summarize the key features and discussion.  Further details can be found in the linked sections below.

The two most unusual journal features we intend to try:

[Reviewer abstracts](https://woozy-page-39c.notion.site/2e9398af2ab0816ea6cdf847f6447b46?pvs=25#2e9398af2ab08184aaa1e2f9d411e99f): For each accepted paper, reviewers will jointly write a public, reader-oriented condensed review. This is meant to convey strengths, caveats, audience fit, and relationship to prior work—i.e., the information that is usually lost when review output is compressed to an accept/reject decision.

[Reviewer compensation](https://woozy-page-39c.notion.site/2e9398af2ab0816ea6cdf847f6447b46?pvs=25#2e9398af2ab081b28b3ef31f11296bbb): Reviewers will be paid, with compensation calibrated to encourage thoroughness and speed (and with an additional incentive for writing the reviewer abstract). We intend to treat this as an incentive-design experiment and adjust the scheme based on observed outcomes.

Furthermore, we are tentatively planning on adopting the following:

[Archival](https://woozy-page-39c.notion.site/2e9398af2ab0816ea6cdf847f6447b46?pvs=25#2e9398af2ab0810ca13df7134e5882f1): The authors cannot publish the same work in another journal or conference. (Pre-prints like on the arXiv are fine of course.)

[Semi-confidential review](https://woozy-page-39c.notion.site/2e9398af2ab0816ea6cdf847f6447b46?pvs=25#2e9398af2ab0812e8858ee720ed39011): During the review process, reviewers will be anonymous and the discussion will be confidential. Upon acceptance, reviewer names are published by default and they may sign the reviewer abstract.

[Web-first open formatting](https://woozy-page-39c.notion.site/2e9398af2ab0816ea6cdf847f6447b46?pvs=25#2e9398af2ab08155bc93d7ee2dbaf693): PDFs will be available, but articles will be formatted with the expectation they will be primarily consumed through a web browser, and released under CC-BY.

In addition we will meet as many traditional institutional requirements as possible to encourage academic participation:

DOI records for canonical article discovery

ORCID-type identifiers for researchers

Reputable editorial board

Review is a large investment of expert time, a precious resource, and it is substantially wasted when the publicly available output of the review is compressed to a single bit (accept or reject).  Public review avoids this, but introduces additional problems due to lack of confidentiality: less honest, more combative and defensive conversations between authors and reviewers. Public review also produces an artifact that is poorly suited to a reader because the conversation may meander, involving disagreements that are only resolved later, etc.

Our experimental solution to address this problem is to publish each accepted paper with a “reviewer abstract”.  Its main goal is to help a potential reader decide — on the paper’s merits — if the paper is worth reading.  It is slightly reminiscent of the “Paper Decision” paragraph on OpenReview (e.g., for NeurIPS), but it is much more extensive and optimized for the potential reader, rather than being merely a terse justification of the decision. We have been very pleased with the reviewer abstracts from the ODYSSEY

Each published paper appears with a

traditional abstract written by the authors; and

a “reviewer abstract” written by one or more of the reviewers.

Authors do not have to endorse the reviewer abstract, but the reviewer abstract must be accepted by the authors in this weak sense: if the authors cannot convince the reviewers or editors to make a modification, then the authors can withdraw their paper altogether, so nothing is published.

(Similarly, when a reviewer recommends a paper for publication, they are essentially finding that the conventional author-written abstract, and the paper as a whole, as acceptable to publish, but are not understood to endorse it.)

As authors withdrawing their paper at the post-acceptance stage of the review process would be very unfortunate, the editor should make a strong effort to find compromise wording that both authors and reviewers can live with. Bringing in other editors in these rare circumstances may be appropriate.

About 1–3 paragraphs, depending on reviewer effort/interest

Editor asks one reviewer to write the reviewer abstract, but the reviewer is free to incorporate material from the entire review process

Less burdensome for reviewers than it appears: Often, the first paragraph of each reviewer’s report is already similar to an abstract, and many machine learning conferences already require reviewers to summarise the paper before criticising.

just perfunctorily summarize the paper; or

just give one-dimensional assessment of paper quality

Intended to be the abstract a potential reader would most want to read before reading the full paper

Summarizing the paper’s contents, but also

Give caveats, strengths, weaknesses, implications, and relationship to prior art

Identify which readers are likely to find reading it worthwhile

All reviewers have the option to sign the reviewer abstract with their name or, with the discretion of editor, anonymously

Anonymous signatures still lend some credence to the review, insofar as journal has reputation for picking good reviewers

We experimented with reviewer abstracts for the Proceedings of ODYSSEY, the [2025 instantiation of the ILIAD conference](https://www.iliadconference.com/) series. One reviewer was offered $100 (on top of the payment for their review) to synthesize the review discussion into an abstract. See the appendix for real reviewer abstracts produced by this review, along with the instructions we gave.

It’s a perennial editorial challenge to motivate reviewers to deeply read papers, write thorough reports, and submit them promptly.  To that end, we intend to launch with an experimental reviewer compensation program, most likely paying reviewers roughly $500–$2,000 to review a paper. The payment scale will be developed adaptively and iteratively, but an appropriate target reference amount could depend on

paper length/difficulty (include appendices?),

Reviewer seniority/credentials(?),

Editor discretion(?) (but conflict of interest potential)

We will furthermore offer a bonus to the reviewer who writes the reviewer abstract.

We will treat this as an incentive experiment: incentive design is hard, and we expect to calibrate, iterate, and—if we observe perverse effects—pause or scrap reviewer payments altogether. Indeed, platforms such as Stack Overflow have repeatedly adjusted reputation, bounties, and badge thresholds to reduce gaming and incentivize the production of actually useful content; we expect a similar need to tune our parameters.

We think it’s very reasonable to spend an average of ~$3k per paper on reviewer payments.  This is especially true because we will produce a public written artifact, the reviewer abstract. By comparison, the typical paper in the US costs $50k–$200k to produce (inclusive of researcher salary), and journals with open-access fees typically charge $1–5k just for publication.

The exact payment schedule will evolve in response to feedback and measured outcomes (review timeliness, inter-editor quality agreement, and author satisfaction). For concreteness, here’s one starting point:

Base pay: $100 + $20/page (excluding appendices)

Quality multiplier (for an “excellent” review): 2x

Speed bonus: $100 per week it’s submitted before the nominal 4-week deadline.

Reviewer abstract bonus: $300

With this schedule, a standard-quality review of a 20-page paper submitted within 3 weeks would earn $600.  An excellent review of the same paper submitted within 2 weeks that was selected for the reviewer abstract would earn $1500. More sophisticated mechanisms could be devised, such as dividing a pot of reviewer rewards based on other reviewer’s opinions of a given review; the reviewer must do well in the eyes of their own peers.

We recognize that increasing review payments with paper length will incentivize the editors to favor short papers over long ones, but we think this is if anything a feature, not a bug. Holding review quality fixed, the burden on a reviewer scales with paper length, roughly linearly.  Editors relying on unpaid reviewers are probably not treating the reviewer effort to review long papers with enough care. We will mitigate edge cases (e.g., long appendices) with “effective page length” guidelines or caps if needed.

Incentivize quality and promptness

Skeptical expert reviewers more willing to take risk, less likely to think they’re wasting their time with new goofball journal

Supports some researchers to specialize more in review. In our opinion, it would be bad to have full-time reviewers (who had thus stopped doing their own research), but researchers should probably distribute broadly over the range 0–25% of worked hours spent reviewing.

Reduces output and quality by undermining a sense of service and trust (e.g., the blood donation and Israeli day care examples)

Reviewers get upset about the editor’s judgement of quality, feel review compensation is tied to whether review is positive vs. negative, etc. Might waste time or social capital on disputes

Looks weird or untrustworthy to outsiders, e.g., just another way for rationalists to pocket Open Philanthropy money

Introduces another potential conflict of interest: editors may be biased to compensate reviewers based on personal relationships

Some reviewers will be prohibited from accepting compensation, e.g., most of those who work for government organizations like UK AISI.

For context, we experimented with offering payments to reviewers for the [ODYSSEY conference proceedings](https://www.iliadconference.com/proceedings), although because the review process hasn’t finished we have not yet issued the payments nor surveyed the participants on their feelings toward it.  Here was the payment schedule (no speed bonus or length scaling):

$200 for “useful” reviews

+$200 for “excellent” reviews (so $400 total)

+$100 for writing the reviewer abstract (so $500 max)

Thus the total cost per paper was: ~$850 = 2.5 reviews at $300/review average + $100 reviewer abstract

A preliminary observation was that, although the initial reviewer reports were prompt, the post-conference follow-up responses were slow in comparison, perhaps suggesting benefits to tying compensation to full conversation speed or quality. However, this was hard to disentangle from motivation provided by conference deadlines.

The Alignment journal will place a high emphasis on matching papers to reviewer that have appropriate skills, motivation, and background to ensure each paper is read deeply, proofs and conceptual arguments are checked carefully, etc. High-quality matching, especially in early stages when the community is small, is ultimately a social phenomenon; it requires hard work by editors and strong and continuous engagement with the community to find reviewers and convince them that we are putting their effort to productive use and rewarding them for their work. On the mechanistic side, we have some tricks up our sleeve:

We will import reviewer and author profile information from OpenReview to the extent allowed.

We intend to experiment with LLM recommendations to surface candidates that might not be salient to the editors.

We aim to make the review conversation (between authors and reviewers) lower friction and faster turnaround that the traditional conference/journal review process:

We hope that reviewer bonuses based on speed will incentivize them to review quickly (and respond to author rebuttals quickly, etc.). Likewise, we aim to optimize the UI frictionless conversation.

We will allow authors to ask clarifying questions before they submit their first full review. Hopefully this will be relatively infrequent, as it indicates a problem with the paper’s presentation, but we want to avoid reviewers wasting time writing a long report based on a misconception

A potential pitfall of this is that (1) the professionalism degrades and/or (2) reviewers or authors have their time wasted by an interminable conversation. We think these risks are manageable.

The review process at journals, conferences, and workshops can adopt varying levels of confidentiality for the reviewers’ identities and the review discussions. Beneficially, confidential review…

can promote honest conversation

avoids professional reprisal by authors on reviewers (especially junior reviewers)

permits lazy/poor review without consequences

lacks transparency; reader can’t dig into dispute themselves

We are planning to adopt a semi-confidential review process.  Here is one tentative proposal, although this may change.

Author identity known to reviewers

(Double-blind peer review seems hopeless in the age of preprints)

Reviewer identity hidden from authors during the review process

The review conversation between authors and reviewers is confidential

If the paper is accepted:

The output of review, the reviewer abstract (see above), is published publicly

The abstract can be signed by some or all of the reviewers

By default, reviewer identities are revealed.

In exceptional circumstances, editors may grant reviewers permanent confidentiality.

If the paper is rejected:

The authors may optionally choose to have the review conversation made public if, e.g., they believe the quality was low.

Reviewer identity remains confidential.

Any reviewers may, if desired, adapt reviewer comments (but not author comments) and post on other forums or social media.

This is subject to revision in response to community feedback and observed performance. In particular, we’re cognizant that (like traditional journals) reviewers could still potentially torpedo good papers unjustly without being revealed.

We are tentatively planning on making the journal archival, meaning that publication there constitutes the “version of record”, in contrast to a workshop publication. (Preprints of course are allowed.)

Greater respect for archival publications

When readers search for a good paper, they will find it associated with this journal

If we have a quick review process, we can produce the “version of record” fast to avoid citation fragmentation

Researchers wary of a new journal may be less likely to take a chance on it

Authors can’t get our value-add (constructive review) without accepting a constraint (ability to publish elsewhere)

The review process will be done using a manuscript in PDF format, which can be generated by the authors using whatever software they prefer (e.g., LaTeX). This avoids wasting the time of authors of papers that are later rejected.

Following acceptance, authors may pass their manuscript to the journal in any reasonable format (LaTeX or markdown preferred; Word and PDF acceptable).

This allows reflowable text and mobile readability.

We currently do not plan to support interactive content, as we do not think the large effort is worth the modest benefit.

We expect to have significant resources (staff and software) available to make the conversion.

Our highest priority will be to avoid wasting author time. We’re very cognizant from first-hand experience that poor conversion quality, perhaps requiring back-and-forth with the author, is very unpleasant and a huge time suck.

We may change our mind before launching the journal and go with the easiest option of directly posting author-generated PDFs.

PDFs will be available on the website for readers who prefer that format.

Published work, including reviewer abstracts, will be released under a [CC-BY](https://creativecommons.org/licenses/by/4.0/) license, in agreement with community norms, and in recognition that most of the research is publicly or philanthropically funded.

We probably wish to be a Diamond Open Access journal; this would place us in the content of a larger open access movement, plus some funding agencies incentivize participation in such schemes. See the [Appendix](https://woozy-page-39c.notion.site/2e9398af2ab0816ea6cdf847f6447b46?pvs=25#2e9398af2ab081169e55e81a421276d0) for the detailed Diamond Open Access criteria, which are easy to meet.

All publications will receive a DOI, as is standard.

The journal review process is ultimately a moderated discussion. Forum Magnum, which powers LessWrong and the Alignment Forum, is potentially a nice software platform for this. (It’s a vastly better user experience than, e.g., OpenReview, especially for editors.) The largest existing community of alignment researchers also frequents the site.

We would like the Alignment journal to enjoy some of the benefits of the Alignment Forum (AF) and social media generally while avoiding the downsides of those platforms as much as possible.

We have had significant discussions with Oliver Habryka about a possible integration with Alignment Forum.  Ideas from that discussion are described here.  Neither Lightcone nor the Alignment Journal has committed to a partnership yet.

Authors submit through a normal online interface.

Submissions to the Alignment journal that are desk-rejected (not sent out for review) never appear on AF.

Many/most reviewers will be recruited through email like a traditional journal.

We will give them the opportunity to submit their reviews through the website, which will have a better user experience than OpenReview.

A minority of reviewers may prefer to submit reviews directly by email, which we will accommodate.

All submissions currently under review will appear, at the least, on a weekly “Alignment Journal round-up” post on AF.

The post will be “stickied” or otherwise visually highlighted

AF users can comment on individual papers there, but the fact that the post is not specific to a single paper makes it feel less like “the” online home for discussion on the paper, so the author will not feel they are obligated to reply to everything.

When submitting to Alignment Journal, authors can check a box to have a dedicated single-paper post created on AF.

The round-up post can link to this.

Authors can choose the moderation policies on their post.

For any paper currently under review, AF forum users will be able to self-nominate as reviewers with one click.

This will provide the editor with a selection of reviewers who have reputations in the AF community and (crucially) who have indicated interest in reviewing the paper.

Potential reviewers from AF will have the identity known to the editors, and will be assessed for relevant expertise based on training and research record like all other reviewers.

In order to encourage balance, we may institute a rule or guideline that at most one reviewer will be sourced from AF per paper. Note that many ML conferences offer a bidding or request feature that allows self-nomination from within a pool of attendees, and it’s generally considered fine to leave this to editor judgement.

The editor chooses the reviewers, and the journal review process proceeds as normal, with the paper either accepted or rejected.

If a submission is published in the journal, the AF post is updated to reflect this, and the reviewer abstract is added. The reviewer abstract can be upvoted on AF, with the reviewers with AF accounts who sign the abstract receiving karma as appropriate.

If a submission is rejected, the AF post (with comments) remains on the website, but it is de-emphasized, and the “under review” statement is removed. (Thus, we don’t announce rejection, but it can be inferred with active effort. Authors will be made aware of this when submitting.)

Authors of accepted papers may, if they wish, ask that their Alignment journal publication link to the AF or another respected online forum.

One can imagine various issues or biases that could arise with the above procedure, but we think they can be mitigated through iterative changes.

Conferences dominate over journals in machine learning, but we decided on a journal. Conferences differ from journals mainly in that conferences…

have an in-person event, strengthening the research community; and

have periodic (e.g., yearly) submission deadlines. This gives two advantages:

An artificial deadline pushes authors and reviewers to complete work rapidly, which largely why journal review is typically slower.

Parallel review of many papers simultaneously allows reviewers to be (partially or fully) assigned from the pool of submitting authors, making the editor’s job easier.

Our main reasons to choose a journal over a conference are

We want to make review and dissemination fast, which conflicts with waiting for a yearly deadline.

This is less of a problem if there are many conferences on the same topic, but we want to create a home for a topic (alignment) that does not have a good home in other conferences.

Organizing an in-person event requires a lot of additional work beyond our main goal, which is review. This can always be added later.

The simultaneous review benefit is less useful to us because we want to draw strongly on outside reviewers, increasing integration.

We think we can get some of the motivation benefit of deadlines by simply creating some artificial deadlines, e.g., periodically (every 3 or 6 months) highlighting the best papers with a badge like “Editor’s selection.” Then you could say “submit by June 1st to be eligible for this quarter’s editor’s selection!”

The journal will be philanthropically funded. We are grateful to the contributors to the [AI Safety Tactical Opportunities Fund](https://manifund.org/JueYan), a pooled multi-donor fund run by JueYan Zhang, for providing our first year of funding. If we can get strong support with the alignment research community, we expect funding will be available for the foreseeable future. It’s harder to be confident in the long term; in principle we would be vulnerable to funder fatigue.

These are some of the objective and quasi-objective indicators that we will use to estimate the success of the journal:

Does the best alignment research get submitted to us?

“Best” is subjective of course, but conditional on whose taste you trust this is pretty measurable.

Time between submission and publication. We expect things to start slow as we work out the kinks in the process, but our medium- and long-term goals are something like 2 and 1 month.

An intermediate indicator is “what fraction of the time spent on the review process is waiting on the editor or the reviewers?”, since we have more control over this number than on how long it takes authors to respond. But ultimately the goal is to make everything go fast.

Impact factor and related traditional journal metrics.

When surveyed, do authors report that the reviews they get with us are deeper and more constructive than, e.g., NeurIPS?

Do we convince senior researchers to review for us? (Somewhat hard to measure.)

Do people link the reviewer abstract on social media? Do they get cited?

The Alignment journal will begin as a project fiscally sponsored by [Principles of Intelligence](https://princint.ai/), the parent organization for the PIBBSS Fellowship. Significant support in launching the journal is being provided by [Iliad](https://www.iliad.ac/), which is also a project of Principles of Intelligence.

We are in the process of recruiting for both the editorial board (initial target: ~10 editors covering a range of alignment expertise) and the advisory board (initial target: ~5 advisors).

Jess Riedel, Dan Mackinlay, and Dan Murfet have agreed to be founding members of the Alignment editorial board. Marcus Hutter and Geoffrey Irving have expressed strong interest in joining the advisory board. We are in discussion with Oliver Habryka and Lighthaven about their contributing software engineering work and a partnership with the Alignment Forum.

Dan MacKinlay is a Research Scientist at CSIRO’s Data61 Australia’s national information technology laboratory. He is a founding member of LAIR2, the Melbourne AI Safety Hub, and a PIBBSS research resident at the London Initiative for Safe AI. He has written over one million words online about AI, machine learning, philosophy etc. Links: [Personal Website](https://danmackinlay.name/),  [Google Scholar](https://scholar.google.com.au/citations?hl=en&view_op=list_works&gmla=AJsN-F6qyuWFaMtwCFdQiVkjp5T3YLiw2k7b7JYe_x2Wfzk6kWP5XkB82kp9uy39jVTrOLtHQD0q5jSSl77YNBRxQqG5GeB5nZt9yfdZLYZNnHpy9GEPt8U&user=JRD1yecAAAAJ).

[Iliad](https://www.iliad.ac/) is an applied mathematics AI alignment research and support organization—a fiscally sponsored project of [Principles of Intelligence](https://princint.ai/). It is an organization of approximately 15 in total, including its research collaborators, with an operations team of 4. Iliad ran [ILIAD 2024](https://www.iliadconference.com/iliad-2024), [ILIAD 2025 (ODYSSEY)](https://www.iliadconference.com/), and [Agent Foundations 2025](https://www.agentfoundations2025atcmu.org/). It aims to fold mathematical experts from academia and elsewhere into AI alignment research, with the aim of maturing alignment into a fully developed, rigorous scientific field.

Iliad’s Executive Director is Alexander Gietelink Oldenziel. He is responsible for curating the organization’s research portfolio and designing its theory of change. Caleb Rak and David Udell together run Iliad’s events operations and fundraising.

Alexander Gietelink Oldenziel was formerly the Director of Strategy and Outreach at [Timaeus](https://timaeus.co/). He had a hand in the founding of both Timaeus and [Simplex](https://www.simplexaisafety.com/), and has a long history organizing various research workshops and conferences in AI alignment.

Caleb Rak is a veteran events manager and is Head of Events & Operations at Iliad. He organizes and runs operations for Iliad’s conferences. He has managed the logistics for events by the [Topos Institute](https://topos.institute/), [SPARC](https://www.sparc.camp/), [ESPR](https://espr.camp/), and others.

David Udell is Content Manager at Iliad. He leads conceptual development and fundraising for Iliad’s events and supports operations for them. He has worked in mechanistic interpretability research, was affiliated with Team Shard, and is a MATS alum.

How could a journal have negative impact?

We could fail to bring status to the field. Publishing low-quality work in an “official” Alignment journal could lead people to think there isn’t value in studying the topic

Could waste the time of alignment researchers without sufficient upside

If we end up with “market power” but run the journal poorly, we could bring the field into disrepute

if we run the journal well, attract much market share but fail to provide a persistent presence for published work, and then we lose funding or the organisaiton otherwise collapses, and our site and contents vanishes from the internet we might “memory-hole” some high-value research

By introduce radical and highly effective tools to academic publishing we could accelerate dangerous research, such as the gain of offense-dominant capabilities

If the journal is successful, it could become a good platform for several possible extensions:

Prizes, which (relative to journal publications) have complementary features for identifying and incentivizing good research

Replication bounties: Offer payments to 3rd parties to do post-publication replication for certain papers that the editors judge are notable/important and are somewhat doubted by the community.

(Full replication of a paper is probably too costly to be done routinely, which is why we think it should not be part of the normal review process.)

Public forecasts (or even prediction markets) about which publications will later achieve some objective correlate of importance like citation count or replication. Some combination of editors, reviewers, or readers could participate.

Retrospective review of existing published papers, either

With author participation (i.e., adversarial)

Without author participation (i.e., 3rd party commentary)

Journal-to-conference track: In agreement with one or more conferences, we could “certify” some high-quality subset of papers we publish to get automatic inclusion in a track at the conferences.

See [TMLR’s blog post](https://medium.com/@TmlrOrg/tmlr-joins-neurips-icml-iclr-journal-to-conference-track-937a898eab3d) on doing this for their journal with the NeurIPS, ICML, and ICLR conferences.

Conference-to-journal track: Can we give a DOI and a seal of approval to some papers, either with full review cycle or just as a side hustle?

ICLR, NeurIPS, ICML, UAI, KDD make reviews public under CC-BY for accepted papers. (They all use OpenReview.) ICLR is public for rejected papers too.

CVPR, ICCV, ACL, EMNLP, AAAI, IJCAI reviews are not public

Review articles, pedagogy, and opinion

Perhaps commissioned, i.e., author payments

These might feature stronger editorial input and perhaps interactive content, but cf. the [Distill postmortem](https://distill.pub/2021/distill-hiatus/) and comparison in the Appendix.

Below is a list of papers that would be considered in-scope.  They are organized crudely into categories to aid the reader, but of course many could be listed in multiple categories. The category titles are not meant to suggest that any research within these categories would be appropriate for the journal.

Paul Christiano, Buck Shlegeris, and Dario Amodei. “Supervising strong learners by amplifying weak experts” (2018) https://arxiv.org/abs/1810.08575

Ryan Carey, Eric Langlois, Chris van Merwijk, Shane Legg, and Tom Everitt, “Incentives for Responsiveness, Instrumental Control and Impact” [https://arxiv.org/abs/2001.07118](https://arxiv.org/abs/2001.07118)

MacDermott, Matt, Tom Everitt, and Francesco Belardinelli. “Characterising Decision Theories with Mechanised Causal Graphs” (2023). [http://arxiv.org/abs/2307.10987](http://arxiv.org/abs/2307.10987)

Tobias Wängberg, Mikael Böörs, Elliot Catt, Tom Everitt, and Marcus Hutter, “A Game-Theoretic Analysis of the Off-Switch Game” [https://arxiv.org/abs/1708.03871](https://arxiv.org/abs/1708.03871).

Joar Max Viktor Skalse, Lucy Farnik, Sumeet Ramesh Motwani, Erik Jenner, Adam Gleave, Alessandro Abate, “STARC: A General Framework For Quantifying Differences Between Reward Functions” [https://openreview.net/forum?id=wPhbtwlCDa](https://openreview.net/forum?id=wPhbtwlCDa)

Rohin Shah, Vikrant Varma, Ramana Kumar, Mary Phuong, Victoria Krakovna, Jonathan Uesato, and Zac Kenton, “Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals” [https://arxiv.org/abs/2210.01790](https://arxiv.org/abs/2210.01790)

Evan Ryan Gunter, Yevgeny Liokumovich, and Victoria Krakovna, “Quantifying Stability Of Non-Power-Seeking In Artificial Agents” [https://arxiv.org/pdf/2401.03529](https://arxiv.org/pdf/2401.03529)

Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch, “Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments” [https://arxiv.org/abs/1706.02275](https://arxiv.org/abs/1706.02275).

Jakob N. Foerster, Richard Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and Igor Mordatch, “Learning with Opponent-Learning Awareness (LOLA)” [https://arxiv.org/abs/1709.04326](https://arxiv.org/abs/1709.04326).

Dima Ivanov, Paul Dütting, Inbal Talgam-Cohen, Tonghan Wang, David C. Parkes, “Principal-Agent Reinforcement Learning: Orchestrating AI Agents with Contracts” [https://arxiv.org/abs/2407.18074](https://arxiv.org/abs/2407.18074)

Evan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, Scott Garrabrant, “Risks from Learned Optimization in Advanced Machine Learning Systems" [https://arxiv.org/abs/1906.01820](https://arxiv.org/abs/1906.01820)

Kola Ayonrinde, Louis Jaburi, “Evaluating Explanations: An Explanatory Virtues Framework for Mechanistic Interpretability -- The Strange Science Part I.ii” [https://arxiv.org/abs/2505.01372](https://arxiv.org/abs/2505.01372)

The DIAMAS project [lists](https://diamas.org/diamond-open-access) the following requirements to be classified as a Diamond Open Access journal.

Persistent identification: the journal should have a valid and confirmed ISSN.

Scholarly journal: the journal should be a scholarly journal that selects papers via an explicitly described evaluation process before and/or after publication, in line with accepted practices in the relevant discipline ([Diamas Consortium, 2024](https://zenodo.org/doi/10.5281/zenodo.12179619)).

Open Access with open licences: all outputs of the journal should be Open Access and carry an open licence that is included in the article-level metadata. [CC-BY is preferred](https://toolsuite.diamas.org/use-open-licenses-open-access-publishing).

No fees: publication in the journal is not contingent on the payment of fees of any kind (e.g. article processing charges or membership dues). The journal should state this as such on its webpage. Voluntary author contributions and donations are allowed, if this is not a condition for publication.

Open to all authors: authorship in the journal should not be limited to any type of affiliation. Any author can submit an article that is in line with the aims and scope of the journal.

Community-owned: the journal title must be owned by public or not-for-profit organisations (or parts thereof) whose mission includes performing or promoting research and scholarship. These include but are not limited to research performing organisations (RPOs), research funding organisations (RFOs), organisations connected to RPOs (university libraries, university presses, faculties, and departments), research institutes, and scholarly societies. The journal should explain its ownership status on its webpage.

This section contains our thoughts on a few particular existing ideas for reforming journals. We hope it gives a sense of how we think about these things.

Distill was a highly regarded, widely read, and unusual ML journal which focused on clear communication and pedagogy. It went on indefinite hiatus after less than five years of operations, and [their post-mortem](https://distill.pub/2021/distill-hiatus/) is strongly recommended reading. In our assessment, Distill had two main value-adds which, by the editor’s own assessment, were extremely labor-intensive:

Intense (40+ hour) mentorship to authors; and

web-native interactive and dynamic visualizations.

We believe these are worthy goals, and we hope that Distill or another journal finds a way to take up this mantle again in a sustainable way; indeed, LLMs potentially greatly reduce the burden of dynamic content. However, the primary goal of the Alignment journal is clear: efficient and legible assessment of research. We will focus on optimizing the use of reviewer time and effort, by incentivizing good reviews and by extracting the maximum useful signal for potential readers.

If the Alignment journal overall is successful, we could imagine branching out to more pedagogical work with stronger editorial input and interactive content (among many other possibility; see “Further extensions”), but that would only be done with great caution.

[Kim et al.](https://arxiv.org/pdf/2505.04966v1) suggest providing LLM-generated reviews to the authors, but not reviewers, as part of the review process.

> The inclusion of LLM-generated reviews serves two main purposes: (1) LLM reviews act as a psychological deterrent against the few irresponsible reviewers who might otherwise rely entirely on LLMs for evaluations, as they know the System is already incorporating such automated reviews, and (2) it provides authors with a soft reference point to identify and flag potential LLM-generated reviews, detailed in our second proposal.

We do not find this compelling. Authors can easily obtain their own reviews from any of the commercially available LLMs. The issue with LLM input into reviews is not detecting it or dissuading authors from using it; we hope and expect most reviewers use LLMs for tasks that LLMs do well, and furthermore that this set of tasks will grow over time. The issue is with authors relying on LLMs inappropriately, i.e., to handle tasks LLMs currently struggle with. Using current LLM stylistic quirks (like em dashes and “it’s not X, it’s Y” structure) to detect lazy reviewers is very unlikely work for more than a short while, and does not address the real problem: incentivizing reviewers to produce high-quality reviews.

If anyone it’s the editors, not authors, who should have experience dealing with reviews that suffer from reckless LLM usage (or other problems). This importantly relies on reading many such reviews, so that one builds up an intuition about their typical problems. It’s not a task that should be pushed onto the author.

As part of “meta-review”, Kim et al. suggest collecting author ratings of reviewer reports, an old idea we expect is modestly useful. They go on to propose that the rating be based only on the author’s reading of the positive parts of the report:

> We propose a sequential release of review content rather than the conventional simultaneous disclosure of all reviews and ratings. Specifically, we divide review content into two distinct sections: section one includes neutral to positive elements, including paper summary, strengths, and clarifying questions, while section two contains more critical parts such as weaknesses and overall ratings. Between these releases, authors evaluate reviews from section one based on two key criteria: (1) the reviewer's comprehension of the paper and (2) the constructiveness of questions in demonstrating careful reading. During this feedback phase, authors can also flag potential LLM-generated reviews by comparing them with the provided LLM reviews. This two-stage disclosure prevents retaliatory scoring while providing the minimal safeguards necessary for a fair review. Once authors complete their feedback, section two is promptly disclosed, and the authors are not allowed to modify their evaluations.

We find this idea interesting, but we worry it will generate more overhead and complications than it’s worth. It’s true the beginning of a reviewer’s report often functions in part to demonstrate that the reviewer understands the material, but we don’t wish to create a separate new component that essentially functions as an author-judged (and probably LLM-gameable) test of reading comprehension. If we decide to collect author rating of reviewer reports, it seems better to statistically correct (condition) them on the reviewer’s rating of the manuscript than to force the reviewer to adopt a particular decomposition of the review into a positive and negative parts where the former is supposed to avoid leaking information about the latter.

Finally, Kim et al. propose tracking reviewer quality and recognizing high-quality reviewers with badges (top 10% and top 30%). We generally support this; gathering data on reviewer quality is prudent, and we plan to do so insofar as it can be collected with minimal burden on the authors and reviewers. That said, we expect it will be hard (not impossible) to create badges or awards that will be strongly valued by the community. This is especially true for a new journal, as opposed to the major ML conferences to which Kim et al.’s ideas are addressed. Instead, we expect that signed reviewer abstracts will be a much more powerful and immediately valued source of reviewer recognition. Whenever possible: show, don’t tell.

Below are two reviewer abstracts for papers submitted to the [Proceedings of ILIAD (2025): ODYSSEY](https://www.iliadconference.com/proceedings), alongside the author abstracts. Even when author abstracts are well-written and hype-free, the reviewer abstract provides substantial depth, contrast, and perspective for the reader. We also include the instructions given to the writer of the reviewer abstract at the end.

Author abstract: We establish that randomly initialized neural networks, with large width and a natural choice of hyperparameters, have nearly independent outputs exactly when their activation function is nonlinear with zero mean under the Gaussian measure: $\mathbb{E}_{z \sim \mathcal{N}(0,1)}[\sigma(z)]=0$. For example, this includes ReLU and GeLU with an additive shift, as well as tanh, but not ReLU or GeLU by themselves. Because of their nearly independent outputs, we propose neural networks with zero-mean activation functions as a promising candidate for the Alignment Research Center's computational no-coincidence conjecture—a conjecture that aims to measure the limits of AI interpretability.

Reviewer abstract: This paper explores which neural architectures have the property that random infinite-width neural networks behave like random functions. Existing results in the theory of random neural networks show that, given a set of $m$ inputs, the distribution preactivations at a layer $\ell$ (over random choice of neural network) is approximately an $m$\-dimensional Gaussian distribution. In order for a neural network to behave like a random function, the covariance matrix of this distribution must be the identity matrix. The authors show that this happens whenever the activation function has an expected value of 0 under the Gaussian measure (e.g., the $\tanh$ function). This is a fairly straightforward extension of known results, but one that is nonetheless useful.

The authors use this result to propose a "no-coincidence conjecture" for neural networks. Loosely speaking, their conjecture states that if a neural network $C: \mathbb{R}^{2n} \to \mathbb{R}^n$ with $\tanh$ nonlinearities has the rare property that no input in $\{-1, 1\}^{2n}$ maps to an all-negative output, then there is a concise structural explanation of $C$ that explains this property. This conjecture extends previous work by Neyman et al., which proposed a similar no-coincidence conjecture in the context of reversible Boolean circuits. The authors argue that their conjecture, if true, suggests a concrete objective for mechanistic interpretability: any full mechanistic explanation of a neural network ought to explain surprising properties like the one they propose.

Author abstract: Deep neural networks trained on vast datasets achieve strong performance on diverse tasks. These models exhibit empirical neural scaling laws, under which prediction error steadily improves with larger model scale. The cause of improvement is unclear, as strong general performance could result from acquiring general-purpose capabilities or specialized knowledge across many domains. To address this question theoretically, we study model scaling laws for a capacity-constrained predictor that optimally instantiates task-specific or general-purpose latent circuits. For a data distribution consisting of power-law-distributed tasks, each represented by a low-dimensional data manifold, general capabilities emerge abruptly at a threshold model scale and decline in relative importance thereafter. Data diversity and model expressivity increase general capabilities in distinct ways.

Reviewer abstract: This paper introduces a novel phenomenological model for scaling laws for general intelligence.

There are two primary modeling innovations. The first builds on earlier work which proposed that data for general intelligence is drawn from a power-law distribution over tasks, but where each task was either learned or not; in this work, each task is itself modeled as a regression task on a $D$\-dimensional manifold. The second innovation is in the learning model itself: the paper presumes that a model has a fixed capacity $N$, and that the model can learn either per-task feature circuits at a cost of $1$, or general circuits that help all tasks simultaneously at a higher cost.

An approximately optimal solution is derived via algebra and a few approximations, and computational experiments are performed that demonstrate that actual solutions are close to expected.

The model is purely a phenomenological model of allocation of capacity to circuits at optimality and the resulting loss as a function of model capacity; nothing is actually "learned", and training dynamics, data availability, and training and inference time compute are not modeled.

The most important and potentially surprising observation is that general capacities emerge abruptly --- for small $N$ the model allocates its capacity entirely to per-task circuits, but above some threshold the model suddenly allocates a large fraction of total capacity to general circuits.

Additionally, above the threshold, model loss drops more sharply (remember that this is a function of capacity $N$, not a "during training" phenomenon). General capacities are much more important when the tail of the power law means there are more rather than fewer tasks (this seems more like a sanity check than a surprise).

The paper is somewhat heavy on algebra and relative to intuition. Some behavioral features of the model are not elucidated, and in particular the paper does not address why a large number of general circuits suddenly appear above a threshold capacity. It is still unclear what model parameter are values are reasonable, what the practical implications are, and the extent to which we should or should not believe this is a "good" model for general intelligence, based on both our intuitions and existing empirical work.

These are the instructions given to the reviewer asked to write the review

> The reviewer abstract is intended neither to perfunctorily summarize the paper nor simply issue a one-dimensional assessment of the paper's importance. Rather, it aims to be the abstract a potential reader would most want to read before reading the full paper. In addition to summarizing the paper's contents, the reviewer abstract should (as appropriate) contain caveats, strengths, weaknesses, implications, and relationship to prior art. It should help a potential reader decide — on the paper’s merits — if the paper is worth reading.

> Please keep the tone professional and respectful, and avoid overall assessment (positive or negative). If something is weak, describe the deficiencies explicitly rather than saying “X is weak”. The point is not to convince the reader to read or not read the paper based on them trusting your/our reputation, but rather to explain the aspects of the paper that have lead you to your assessment, so that the reader can make their own decision. (If you think a paper is really great, or really bad, you are of course free to praise or downplay it separately on social media -- hopefully while linking to the reviewer abstract!)

> You may freely use, mix, and combine any of the text in the review process, including from the authors and other reviewers. We suggest 150-500 words, using multiple paragraphs if appropriate.

> The reviewer abstract is written by a reviewer and accepted by the authors and editors, possibly after requested changes. This mirrors the normal abstract, which is written by the authors and approved by the reviewers and editors, possibly after requested changes.

> For expediency’s sake, the editor choses only one reviewer to write the reviewer abstract. Any of the reviewers may choose to publicly sign the reviewer abstract once it is finalized. (This of course will mean their identity as one of the reviewers become known.)

Here we give an expanded assessment of the weaknesses and strengths of the existing venues for alignment research.

Note that when we refer to review at journals and conferences, we always are discussing review after the manuscript has been released publicly (e.g., posted on the arXiv), which is sometimes known as “post-publication” peer review (despite the work not yet appearing in the journal) because this uses the original sense of “publication” in the sense of widespread dissemination. We do not bother to address “pre-publication” peer review, where passing review is made a condition of public release, because we consider this old-fashioned approach clearly and strictly worse, although it continues to persist in some field.

Extremely slow! (12+ months)

If confidential: Massive reviewer effort (the report) boiled down to a single bit (!)

Difficult to find experts with appropriate expertise and sufficient time to review.

Just 2–3 reviewers, a number that is generally fixed independently of paper importance

Many potential criticisms of papers are “NP” (can be checked easily), so credentials of reviewer should be irrelevant

Shifts influence to senior researchers

No appeal of desk-rejection

Modern innovations often eschewed out of inertia

Anonymous reviewers can be lazy/bad without much consequence

Typically paywalled, so difficult to discover research

Low salience to the ML conference ecosystem, which is conference-oriented

Slow, with approximately 6 month submission→ acceptance pipeline in the best case

If not confidential: reviewer-author conversation is often sprawling, confused, and not optimized for a potential reader (similar to forums)

Since review burdens are high and authors are simultaneously reviewing and rebutting reviews, review quality is anecdotally low

Anonymous reviewers can be lazy/bad without much consequence

Pressure to prioritize “Gain of capability” research, where SOTA results are the fastest track to success

Outside the the AI/ML field may fail to be legible (”just a conference publication”) and thus hostile to computer-science-adjacent research fields that are still important for AI Alignment

Participation is expensive (registration fees and in-person attendance)

Peer review is weak or absent, sapping credibility

Workshops are low-status and rarely count towards academic career KPIs even inside AI/ML research

Still regarded as sub-par outside the the AI/ML field and thus hostile for computer-science-adjacent research

Expensive to participate in, requiring expensive registration fees and in-person attendance

Often fails to attract best commentators

Vast majority of experts have no interest in getting in online fights; you only see the ones that do

Deep readings rare; often just surface level, or topic du jour, or what interests the commentator

Problem becomes worse as papers become more technical

Lacks clear, legible certification

Fashion, piling on, crowd effects; every comment an attack surface

Poor filtering/weighting by expertise

Discussion often side-tracked unless there is excellent (costly) moderation

Many tricks for this (community reputation, karma, etc.), but still very imperfect

Commentators (and author) may be asynchronous, and so not get sufficient back-and-forth

Significant effort spent rebutting papers that are obviously bad to experts and should be desk-rejected

Social media ratings are somewhat correlated with quality, but they quite distinct from careful expert assessment even in the limit of infinite time and views. Joel Mokyr’s “A Culture of Growth” earned him the Nobel prize in economics, but the book is rated [3.9/5 stars on GoodReads](https://www.goodreads.com/book/show/29452523-a-culture-of-growth).

(Often simply the opposite of the above, but noting these explicitly here.)

Higher quality discussion by gating discussion to experts and moderating strongly

Editor recruits for better/more appropriate reviewers (“matching”)

Deep readings are more common (but certainly not guaranteed)

Reviewers feel some responsibility to review overall quality, not just what interests them

More likely to get practical constructive feedback

Applies some pressure for mundane but important changes like better citation, comparison with existing work, more concise writing

Low-cost to desk-reject obviously bad papers

Format has established prestige for established disciplines

Legible and uncontroversial disciplinary sorting is likely to ensure that publication in one of these unambiguously counts towards researcher KPIs, if they are in a classic scientific field

Higher quality discussion by gating discussion to experts and moderating strongly

Algorithmic matching of more appropriate reviewers by sophisticated algorithms (“matching”)

Schelling point for the discussion; reduces async discussion problem

Deep readings are more common (but certainly not guaranteed)

Reviewers feel some responsibility to review overall quality, not just what interests them

More likely to get practical constructive feedback

Applies some pressure for mundane but important changes like better citation, comparison with existing work, more concise writing

Low-cost to desk-reject obviously bad papers

Format has established prestige for ML/AI disciplines

Legible and uncontroversial disciplinary sorting is likely to ensure that publication in one of these unambiguously counts towards researcher KPIs if they are in ML/AI

Fast: approval time is typically 2-3 months

Higher quality discussion by gating discussion to experts and moderating strongly

Face to face discussion eliminates asynchrony

So important. Science is cumulative

Near frictionless; just start typing a comment

“Self-nomination”; editors don’t need to know a reviewer exists and has the interest/energy/time

Review effort scales with paper notability

“Paper” is terrible as a measurement unit of research

“Living”; can identify problems after publication review

Critics who make clearly bad criticism can be punished with lowered reputation (unlike anonymous reviewers)

Journal retractions are much too coarse and rare to serve this purpose
